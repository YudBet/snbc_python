{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bianry train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "from scipy.sparse import spdiags\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def binaryTrain(X, Y, options):\n",
    "    lamda = options['lamda']\n",
    "    k = options['k']\n",
    "    maxIter = options['maxIter']\n",
    "    \n",
    "    _pow = 8\n",
    "    N = X.shape[0]\n",
    "    \n",
    "    trainIdx = Y.nonzero()[0]\n",
    "    Ntr = len(trainIdx)\n",
    "    \n",
    "    wpos = len((Y == -1).nonzero()[0])/float(len((Y == 1).nonzero()[0]))\n",
    "    wneg = 1\n",
    "    \n",
    "    weight = np.zeros(len(Y))\n",
    "    weight[Y == 1] = wpos\n",
    "    weight[Y == -1] = wneg\n",
    "    \n",
    "    if len(Y) != N:\n",
    "        print 'Error: Number of elements in X and Y must same'\n",
    "        return\n",
    "    \n",
    "    degree = np.squeeze(np.asarray(np.sum(X, 1)))\n",
    "    logd = np.log2(2 + degree)\n",
    "    \n",
    "    invD = spdiags(1./degree, 0, N, N).tocsr()\n",
    "    \n",
    "    alpha = 1 - 1./np.log2(2 + degree)\n",
    "    ALPHA = spdiags(alpha, 0, N, N).tocsr()\n",
    "    COMPALPHA = spdiags(1 - alpha, 0, N, N).tocsr()\n",
    "    \n",
    "    M = COMPALPHA * invD * X\n",
    "    \n",
    "    w = np.random.rand(1, X.shape[1])\n",
    "    w = np.matrix(w/float(np.sqrt(lamda)*LA.norm(w)))\n",
    "    \n",
    "    Tolerance=1e-6\n",
    "\n",
    "    for t in range(maxIter):\n",
    "        if (t%100 == 0):\n",
    "            print 'iteration # ', str(t), '/', str(maxIter)\n",
    "        w_old = w\n",
    "        \n",
    "        pred = X*w.T\n",
    "        propogatedPred = np.zeros(shape=pred.shape)\n",
    "        \n",
    "        for i in range(_pow):\n",
    "            propogatedPred = M*propogatedPred + pred\n",
    "        propogatedPred = M*propogatedPred + pred\n",
    "        propogatedPred = np.squeeze(np.asarray(np.multiply(alpha, propogatedPred.flatten())))\n",
    "        \n",
    "        b = np.mean(Y[trainIdx] - propogatedPred[trainIdx])\n",
    "        \n",
    "        idx = np.random.choice(Ntr, k, replace=False)\n",
    "        randomSubset = trainIdx[idx]\n",
    "        \n",
    "        idx1 = (np.multiply(np.sign(propogatedPred[randomSubset] + b), Y[randomSubset]) < 1).nonzero()[0]\n",
    "        \n",
    "        misClass = randomSubset[idx1]\n",
    "        grad = weight[misClass]*Y[misClass]*alpha[misClass]\n",
    "        part = np.matrix(grad.T*M[misClass])\n",
    "        At = csr_matrix(part.shape)\n",
    "        for i in range(_pow):\n",
    "            At = At*M + part\n",
    "        grad = At*X + np.matrix(grad.T)*X[misClass]\n",
    "        grad = np.multiply(lamda*w, logd.T) - 1./k*grad\n",
    "\n",
    "        etat = 1./(2 + lamda*t)\n",
    "        w1 = w - etat*grad\n",
    "        w = min(1, 1./(np.sqrt(lamda)*LA.norm(w1)))*w1\n",
    "        \n",
    "        if LA.norm(w - w_old) < Tolerance:\n",
    "            break\n",
    "    \n",
    "    if t < maxIter - 1:\n",
    "        print 'W converged in ', str(t),' iterations.'\n",
    "    else:\n",
    "        print 'W not converged in ', str(t),' iterations.'\n",
    "     \n",
    "    \n",
    "    pred = X*w.T\n",
    "    propogatedPred = np.zeros(shape=pred.shape)\n",
    "    \n",
    "    for i in range(_pow):\n",
    "        propogatedPred = M*propogatedPred + pred\n",
    "    propogatedPred = M[trainIdx]*propogatedPred + pred[trainIdx]\n",
    "    propogatedPred = np.squeeze(np.asarray((ALPHA[trainIdx][:,trainIdx]*propogatedPred)))\n",
    "\n",
    "    b = np.mean(Y[trainIdx] - propogatedPred)\n",
    "    \n",
    "    \n",
    "    Tr = np.sum(np.sign(propogatedPred + b) == Y[trainIdx])\n",
    "    F = Ntr - Tr\n",
    "    TrainAccuracy = 100.*Tr/(Tr + F)\n",
    "    print 'Accuracy on Training set = ', str(TrainAccuracy),' %\\n'\n",
    "\n",
    "    model = {\n",
    "        'w': w,\n",
    "        'b': b\n",
    "    }\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Snowball sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\" Snowball Sampling \n",
    "    INPUT:\n",
    "        graph:        adj. mat. of a graph\n",
    "        tr_fraction:  fraction of training nodes, default is 0.1\n",
    "    OUTOUT:\n",
    "        Train:        indices of training nodes\n",
    "\"\"\"\n",
    "def snowball_sampling(graph, tr_fraction=0.1):\n",
    "    TrFraction = tr_fraction\n",
    "\n",
    "    graph = graph + graph.T\n",
    "    graph[graph > 0] = 1\n",
    "\n",
    "    sparseGraph = graph.nonzero()\n",
    "\n",
    "    TrFraction = 1 - TrFraction\n",
    "    \n",
    "    Nodes = np.unique(sparseGraph)\n",
    "    n_Test = int(np.floor(len(Nodes)*TrFraction))\n",
    "\n",
    "    Test = np.zeros(n_Test)\n",
    "    Train = np.zeros(len(Nodes) - n_Test)\n",
    "\n",
    "    n_seed = int(np.ceil(n_Test*0.02))\n",
    "    Seed = np.random.choice(Nodes, n_seed, replace=False)\n",
    "    Selected = Seed\n",
    "\n",
    "    while len(Selected) < n_Test:\n",
    "        tmp_Neighbor = Nodes[np.squeeze(np.asarray(np.sum(graph[Seed], axis=0).ravel())).nonzero()[0]]\n",
    "        Neighbor = []\n",
    "        for n in tmp_Neighbor:\n",
    "            if n in Selected:\n",
    "                continue\n",
    "            Neighbor.append(n)\n",
    "\n",
    "        tmp_Selected = []\n",
    "        if len(Neighbor) > 0:\n",
    "            tmp_Selected = np.random.choice(np.array(Neighbor), int(len(Neighbor)*TrFraction/2), replace=False)\n",
    "\n",
    "        if len(tmp_Selected) == 0:\n",
    "            UnSelected = np.setdiff1d(np.array(range(graph.shape[0])), Selected, assume_unique=True)\n",
    "            Seed = np.random.choice(UnSelected, np.min([n_seed, len(UnSelected)]), replace=False)\n",
    "            Selected = np.unique(np.append(Selected, Seed))\n",
    "        else:\n",
    "            Selected = np.unique(np.append(Selected, tmp_Selected))\n",
    "            Seed = tmp_Selected\n",
    "    Test = Selected[0 : n_Test]\n",
    "    Train = np.setdiff1d(np.array(range(graph.shape[0])), Test, assume_unique=True)\n",
    "    return Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test input: amazon dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "mat = scipy.io.loadmat('Datasets/amazon.mat')\n",
    "\n",
    "A = mat['graph'].tocsr()\n",
    "\n",
    "truth = mat['label'].toarray()[:,0] # test on the first label\n",
    "truth[truth == 0] = -1\n",
    "tr = int(len(truth) * 0.1)\n",
    "\n",
    "#tr_idx = np.random.choice(len(truth), tr, replace=False) # random select\n",
    "tr_idx = snowball_sampling(A, tr_fraction=0.1) # snowball sampling\n",
    "\n",
    "y = np.zeros(len(truth))\n",
    "y[tr_idx] = truth[tr_idx]\n",
    "lamda = pow(2, -6)\n",
    "imax = 1000\n",
    "k = 1000\n",
    "\n",
    "options = {\n",
    "    'lamda': lamda,\n",
    "    'k': k,\n",
    "    'maxIter': imax\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test predict: training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = binaryTrain(A, y, options)\n",
    "print 'baseline accuracy = ', 100 - float(len((y == 1).nonzero()[0]))/(len(tr_idx))*100 # acc. of all -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Binary predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def binaryPredict(X, model):\n",
    "    N = X.shape[0]\n",
    "    \n",
    "    w = model['w']\n",
    "    b = model['b']\n",
    "    \n",
    "    degree = np.squeeze(np.asarray(np.sum(X, 1)))\n",
    "    logd = np.log2(2 + degree)\n",
    "    \n",
    "    invD = spdiags(1./degree, 0, N, N).tocsr()\n",
    "    \n",
    "    alpha = 1 - 1./np.log2(2 + degree)\n",
    "    ALPHA = spdiags(alpha, 0, N, N).tocsr()\n",
    "    COMPALPHA = spdiags(1 - alpha, 0, N, N).tocsr()\n",
    "    \n",
    "    M = COMPALPHA * invD * X\n",
    "\n",
    "    _pow = 8\n",
    "    pred = X*w.T\n",
    "    propogatedPred = np.zeros(shape=pred.shape)\n",
    "\n",
    "    for i in range(_pow):\n",
    "        propogatedPred = M*propogatedPred + pred\n",
    "    propogatedPred = M*propogatedPred + pred\n",
    "    propogatedPred = ALPHA*np.squeeze(np.asarray(propogatedPred))\n",
    "    \n",
    "    propogatedPred = propogatedPred + b\n",
    "    \n",
    "    return propogatedPred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bp_res = binaryPredict(A, model)\n",
    "print 'Pre = ', np.unique(bp_res, return_counts=True)\n",
    "print 'Act = ', np.unique(truth, return_counts=True)\n",
    "print np.unique(y[tr_idx], return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multi train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start sampling...\n",
      "End sampling\n",
      "Start training label 0\n",
      "iteration #  0 / 1000\n",
      "iteration #  100 / 1000\n",
      "iteration #  200 / 1000\n",
      "iteration #  300 / 1000\n",
      "iteration #  400 / 1000\n",
      "iteration #  500 / 1000\n",
      "iteration #  600 / 1000\n",
      "iteration #  700 / 1000\n",
      "iteration #  800 / 1000\n",
      "iteration #  900 / 1000\n",
      "W not converged in  999  iterations.\n",
      "Accuracy on Training set =  87.7253731343  %\n",
      "\n",
      "Start training label 1\n",
      "iteration #  0 / 1000\n",
      "iteration #  100 / 1000\n",
      "iteration #  200 / 1000\n",
      "iteration #  300 / 1000\n",
      "iteration #  400 / 1000\n",
      "iteration #  500 / 1000\n",
      "iteration #  600 / 1000\n",
      "iteration #  700 / 1000\n",
      "iteration #  800 / 1000\n",
      "iteration #  900 / 1000\n",
      "W not converged in  999  iterations.\n",
      "Accuracy on Training set =  92.1313432836  %\n",
      "\n",
      "Start training label 2\n",
      "iteration #  0 / 1000\n",
      "iteration #  100 / 1000\n",
      "iteration #  200 / 1000\n",
      "iteration #  300 / 1000\n",
      "iteration #  400 / 1000\n",
      "iteration #  500 / 1000\n",
      "iteration #  600 / 1000\n",
      "iteration #  700 / 1000\n",
      "iteration #  800 / 1000\n",
      "iteration #  900 / 1000\n",
      "W not converged in  999  iterations.\n",
      "Accuracy on Training set =  97.2776119403  %\n",
      "\n",
      "Start training label 3\n",
      "iteration #  0 / 1000\n",
      "iteration #  100 / 1000\n",
      "iteration #  200 / 1000\n",
      "iteration #  300 / 1000\n",
      "iteration #  400 / 1000\n",
      "iteration #  500 / 1000\n",
      "iteration #  600 / 1000\n",
      "iteration #  700 / 1000\n",
      "iteration #  800 / 1000\n",
      "iteration #  900 / 1000\n",
      "W not converged in  999  iterations.\n",
      "Accuracy on Training set =  89.5641791045  %\n",
      "\n",
      "Start training label 4\n",
      "iteration #  0 / 1000\n",
      "iteration #  100 / 1000\n",
      "iteration #  200 / 1000\n",
      "iteration #  300 / 1000\n",
      "iteration #  400 / 1000\n",
      "iteration #  500 / 1000\n",
      "iteration #  600 / 1000\n",
      "iteration #  700 / 1000\n",
      "iteration #  800 / 1000\n",
      "iteration #  900 / 1000\n",
      "W not converged in  999  iterations.\n",
      "Accuracy on Training set =  99.3194029851  %\n",
      "\n",
      "Start training label 5\n",
      "iteration #  0 / 1000\n",
      "iteration #  100 / 1000\n",
      "iteration #  200 / 1000\n",
      "iteration #  300 / 1000\n",
      "iteration #  400 / 1000\n",
      "iteration #  500 / 1000\n",
      "iteration #  600 / 1000\n",
      "iteration #  700 / 1000\n",
      "iteration #  800 / 1000\n",
      "iteration #  900 / 1000\n",
      "W not converged in  999  iterations.\n",
      "Accuracy on Training set =  82.1850746269  %\n",
      "\n",
      "Start training label 6\n",
      "iteration #  0 / 1000\n",
      "iteration #  100 / 1000\n",
      "iteration #  200 / 1000\n",
      "iteration #  300 / 1000\n",
      "iteration #  400 / 1000\n",
      "iteration #  500 / 1000\n",
      "iteration #  600 / 1000\n",
      "iteration #  700 / 1000\n",
      "iteration #  800 / 1000\n",
      "iteration #  900 / 1000\n",
      "W not converged in  999  iterations.\n",
      "Accuracy on Training set =  97.671641791  %\n",
      "\n",
      "Start training label 7\n",
      "iteration #  0 / 1000\n",
      "iteration #  100 / 1000\n",
      "iteration #  200 / 1000\n",
      "iteration #  300 / 1000\n",
      "iteration #  400 / 1000\n",
      "iteration #  500 / 1000\n",
      "iteration #  600 / 1000\n",
      "iteration #  700 / 1000\n",
      "iteration #  800 / 1000\n",
      "iteration #  900 / 1000\n",
      "W not converged in  999  iterations.\n",
      "Accuracy on Training set =  90.2208955224  %\n",
      "\n",
      "Start training label 8\n",
      "iteration #  0 / 1000\n",
      "iteration #  100 / 1000\n",
      "iteration #  200 / 1000\n",
      "iteration #  300 / 1000\n",
      "iteration #  400 / 1000\n",
      "iteration #  500 / 1000\n",
      "iteration #  600 / 1000\n",
      "iteration #  700 / 1000\n",
      "iteration #  800 / 1000\n",
      "iteration #  900 / 1000\n",
      "W not converged in  999  iterations.\n",
      "Accuracy on Training set =  89.8746268657  %\n",
      "\n",
      "Start training label 9\n",
      "iteration #  0 / 1000\n",
      "iteration #  100 / 1000\n",
      "iteration #  200 / 1000\n",
      "iteration #  300 / 1000\n",
      "iteration #  400 / 1000\n",
      "iteration #  500 / 1000\n",
      "iteration #  600 / 1000\n",
      "iteration #  700 / 1000\n",
      "iteration #  800 / 1000\n",
      "iteration #  900 / 1000\n",
      "W not converged in  999  iterations.\n",
      "Accuracy on Training set =  99.0567164179  %\n",
      "\n",
      "Start training label 10\n",
      "iteration #  0 / 1000\n",
      "iteration #  100 / 1000\n",
      "iteration #  200 / 1000\n",
      "iteration #  300 / 1000\n",
      "iteration #  400 / 1000\n",
      "iteration #  500 / 1000\n",
      "iteration #  600 / 1000\n",
      "iteration #  700 / 1000\n",
      "iteration #  800 / 1000\n",
      "iteration #  900 / 1000\n",
      "W not converged in  999  iterations.\n",
      "Accuracy on Training set =  97.3253731343  %\n",
      "\n",
      "Start training label 11\n",
      "iteration #  0 / 1000\n",
      "iteration #  100 / 1000\n",
      "iteration #  200 / 1000\n",
      "iteration #  300 / 1000\n",
      "iteration #  400 / 1000\n",
      "iteration #  500 / 1000\n",
      "iteration #  600 / 1000\n",
      "iteration #  700 / 1000\n",
      "iteration #  800 / 1000\n",
      "iteration #  900 / 1000\n",
      "W not converged in  999  iterations.\n",
      "Accuracy on Training set =  98.9492537313  %\n",
      "\n",
      "Start training label 12\n",
      "iteration #  0 / 1000\n",
      "iteration #  100 / 1000\n",
      "iteration #  200 / 1000\n",
      "iteration #  300 / 1000\n",
      "iteration #  400 / 1000\n",
      "iteration #  500 / 1000\n",
      "iteration #  600 / 1000\n",
      "iteration #  700 / 1000\n",
      "iteration #  800 / 1000\n",
      "iteration #  900 / 1000\n",
      "W not converged in  999  iterations.\n",
      "Accuracy on Training set =  96.4656716418  %\n",
      "\n",
      "Start training label 13\n",
      "iteration #  0 / 1000\n",
      "iteration #  100 / 1000\n",
      "iteration #  200 / 1000\n",
      "iteration #  300 / 1000\n",
      "iteration #  400 / 1000\n",
      "iteration #  500 / 1000\n",
      "iteration #  600 / 1000\n",
      "iteration #  700 / 1000\n",
      "iteration #  800 / 1000\n",
      "iteration #  900 / 1000\n",
      "W not converged in  999  iterations.\n",
      "Accuracy on Training set =  94.7701492537  %\n",
      "\n",
      "Start training label 14\n",
      "iteration #  0 / 1000\n",
      "iteration #  100 / 1000\n",
      "iteration #  200 / 1000\n",
      "iteration #  300 / 1000\n",
      "iteration #  400 / 1000\n",
      "iteration #  500 / 1000\n",
      "iteration #  600 / 1000\n",
      "iteration #  700 / 1000\n",
      "iteration #  800 / 1000\n",
      "iteration #  900 / 1000\n",
      "W not converged in  999  iterations.\n",
      "Accuracy on Training set =  94.7820895522  %\n",
      "\n",
      "Start training label 15\n",
      "iteration #  0 / 1000\n",
      "iteration #  100 / 1000\n",
      "iteration #  200 / 1000\n",
      "iteration #  300 / 1000\n",
      "iteration #  400 / 1000\n",
      "iteration #  500 / 1000\n",
      "iteration #  600 / 1000\n",
      "iteration #  700 / 1000\n",
      "iteration #  800 / 1000\n",
      "iteration #  900 / 1000\n",
      "W not converged in  999  iterations.\n",
      "Accuracy on Training set =  93.4686567164  %\n",
      "\n",
      "Start training label 16\n",
      "iteration #  0 / 1000\n",
      "iteration #  100 / 1000\n",
      "iteration #  200 / 1000\n",
      "iteration #  300 / 1000\n",
      "iteration #  400 / 1000\n",
      "iteration #  500 / 1000\n",
      "iteration #  600 / 1000\n",
      "iteration #  700 / 1000\n",
      "iteration #  800 / 1000\n",
      "iteration #  900 / 1000\n",
      "W not converged in  999  iterations.\n",
      "Accuracy on Training set =  91.7611940299  %\n",
      "\n",
      "Start training label 17\n",
      "iteration #  0 / 1000\n",
      "iteration #  100 / 1000\n",
      "iteration #  200 / 1000\n",
      "iteration #  300 / 1000\n",
      "iteration #  400 / 1000\n",
      "iteration #  500 / 1000\n",
      "iteration #  600 / 1000\n",
      "iteration #  700 / 1000\n",
      "iteration #  800 / 1000\n",
      "iteration #  900 / 1000\n",
      "W not converged in  999  iterations.\n",
      "Accuracy on Training set =  97.5044776119  %\n",
      "\n",
      "Start training label 18\n",
      "iteration #  0 / 1000\n",
      "iteration #  100 / 1000\n",
      "iteration #  200 / 1000\n",
      "iteration #  300 / 1000\n",
      "iteration #  400 / 1000\n",
      "iteration #  500 / 1000\n",
      "iteration #  600 / 1000\n",
      "iteration #  700 / 1000\n",
      "iteration #  800 / 1000\n",
      "iteration #  900 / 1000\n",
      "W not converged in  999  iterations.\n",
      "Accuracy on Training set =  94.2686567164  %\n",
      "\n",
      "Start training label 19\n",
      "iteration #  0 / 1000\n",
      "iteration #  100 / 1000\n",
      "iteration #  200 / 1000\n",
      "iteration #  300 / 1000\n",
      "iteration #  400 / 1000\n",
      "iteration #  500 / 1000\n",
      "iteration #  600 / 1000\n",
      "iteration #  700 / 1000\n",
      "iteration #  800 / 1000\n",
      "iteration #  900 / 1000\n",
      "W not converged in  999  iterations.\n",
      "Accuracy on Training set =  93.7194029851  %\n",
      "\n",
      "Start training label 20\n",
      "iteration #  0 / 1000\n",
      "iteration #  100 / 1000\n",
      "iteration #  200 / 1000\n",
      "iteration #  300 / 1000\n",
      "iteration #  400 / 1000\n",
      "iteration #  500 / 1000\n",
      "iteration #  600 / 1000\n",
      "iteration #  700 / 1000\n",
      "iteration #  800 / 1000\n",
      "iteration #  900 / 1000\n",
      "W not converged in  999  iterations.\n",
      "Accuracy on Training set =  98.1492537313  %\n",
      "\n",
      "Start training label 21\n",
      "iteration #  0 / 1000\n",
      "iteration #  100 / 1000\n",
      "iteration #  200 / 1000\n",
      "iteration #  300 / 1000\n",
      "iteration #  400 / 1000\n",
      "iteration #  500 / 1000\n",
      "iteration #  600 / 1000\n",
      "iteration #  700 / 1000\n",
      "iteration #  800 / 1000\n",
      "iteration #  900 / 1000\n",
      "W not converged in  999  iterations.\n",
      "Accuracy on Training set =  92.4179104478  %\n",
      "\n",
      "Start training label 22\n",
      "iteration #  0 / 1000\n",
      "iteration #  100 / 1000\n",
      "iteration #  200 / 1000\n",
      "iteration #  300 / 1000\n",
      "iteration #  400 / 1000\n",
      "iteration #  500 / 1000\n",
      "iteration #  600 / 1000\n",
      "iteration #  700 / 1000\n",
      "iteration #  800 / 1000\n",
      "iteration #  900 / 1000\n",
      "W not converged in  999  iterations.\n",
      "Accuracy on Training set =  97.1104477612  %\n",
      "\n",
      "Start training label 23\n",
      "iteration #  0 / 1000\n",
      "iteration #  100 / 1000\n",
      "iteration #  200 / 1000\n",
      "iteration #  300 / 1000\n",
      "iteration #  400 / 1000\n",
      "iteration #  500 / 1000\n",
      "iteration #  600 / 1000\n",
      "iteration #  700 / 1000\n",
      "iteration #  800 / 1000\n",
      "iteration #  900 / 1000\n",
      "W not converged in  999  iterations.\n",
      "Accuracy on Training set =  99.3791044776  %\n",
      "\n",
      "Start training label 24\n",
      "iteration #  0 / 1000\n",
      "iteration #  100 / 1000\n",
      "iteration #  200 / 1000\n",
      "iteration #  300 / 1000\n",
      "iteration #  400 / 1000\n",
      "iteration #  500 / 1000\n",
      "iteration #  600 / 1000\n",
      "iteration #  700 / 1000\n",
      "iteration #  800 / 1000\n",
      "iteration #  900 / 1000\n",
      "W not converged in  999  iterations.\n",
      "Accuracy on Training set =  93.8029850746  %\n",
      "\n",
      "Start training label 25\n",
      "iteration #  0 / 1000\n",
      "iteration #  100 / 1000\n",
      "iteration #  200 / 1000\n",
      "iteration #  300 / 1000\n",
      "iteration #  400 / 1000\n",
      "iteration #  500 / 1000\n",
      "iteration #  600 / 1000\n",
      "iteration #  700 / 1000\n",
      "iteration #  800 / 1000\n",
      "iteration #  900 / 1000\n",
      "W not converged in  999  iterations.\n",
      "Accuracy on Training set =  98.6029850746  %\n",
      "\n",
      "Start training label 26\n",
      "iteration #  0 / 1000\n",
      "iteration #  100 / 1000\n",
      "iteration #  200 / 1000\n",
      "iteration #  300 / 1000\n",
      "iteration #  400 / 1000\n",
      "iteration #  500 / 1000\n",
      "iteration #  600 / 1000\n",
      "iteration #  700 / 1000\n",
      "iteration #  800 / 1000\n",
      "iteration #  900 / 1000\n",
      "W not converged in  999  iterations.\n",
      "Accuracy on Training set =  95.2955223881  %\n",
      "\n",
      "Start training label 27\n",
      "iteration #  0 / 1000\n",
      "iteration #  100 / 1000\n",
      "iteration #  200 / 1000\n",
      "iteration #  300 / 1000\n",
      "iteration #  400 / 1000\n",
      "iteration #  500 / 1000\n",
      "iteration #  600 / 1000\n",
      "iteration #  700 / 1000\n",
      "iteration #  800 / 1000\n",
      "iteration #  900 / 1000\n",
      "W not converged in  999  iterations.\n",
      "Accuracy on Training set =  97.3492537313  %\n",
      "\n",
      "Start training label 28\n",
      "iteration #  0 / 1000\n",
      "iteration #  100 / 1000\n",
      "iteration #  200 / 1000\n",
      "iteration #  300 / 1000\n",
      "iteration #  400 / 1000\n",
      "iteration #  500 / 1000\n",
      "iteration #  600 / 1000\n",
      "iteration #  700 / 1000\n",
      "iteration #  800 / 1000\n",
      "iteration #  900 / 1000\n",
      "W not converged in  999  iterations.\n",
      "Accuracy on Training set =  97.8029850746  %\n",
      "\n",
      "Start training label 29\n",
      "iteration #  0 / 1000\n",
      "iteration #  100 / 1000\n",
      "iteration #  200 / 1000\n",
      "iteration #  300 / 1000\n",
      "iteration #  400 / 1000\n",
      "iteration #  500 / 1000\n",
      "iteration #  600 / 1000\n",
      "iteration #  700 / 1000\n",
      "iteration #  800 / 1000\n",
      "iteration #  900 / 1000\n",
      "W not converged in  999  iterations.\n",
      "Accuracy on Training set =  98.9253731343  %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import scipy.io\n",
    "\n",
    "mat = scipy.io.loadmat('Datasets/amazon.mat')\n",
    "\n",
    "A = mat['graph'].tocsr()\n",
    "T = mat['label']\n",
    "\n",
    "print 'Start sampling...'\n",
    "tr = int(A.shape[0] * 0.1)\n",
    "#tr_idx = np.random.choice(len(truth), tr, replace=False) # random select\n",
    "tr_idx = snowball_sampling(A, tr_fraction=0.1) # snowball sampling\n",
    "print 'End sampling'\n",
    "\n",
    "options = {\n",
    "    'lamda': pow(2, -6),\n",
    "    'k': 1000,\n",
    "    'maxIter': 1000\n",
    "}\n",
    "\n",
    "models = []\n",
    "for l in range(T.shape[1]):\n",
    "    print 'Start training label' , l \n",
    "    truth = T.toarray()[:, l]\n",
    "    truth[truth == 0] = -1\n",
    "    \n",
    "    y = np.zeros(len(truth))\n",
    "    y[tr_idx] = truth[tr_idx]\n",
    "    \n",
    "    model = binaryTrain(A, y, options)\n",
    "    models.append(model)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multi predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre =  [-0.58678998 -0.57112883 -0.59482961 ..., -0.77187636 -0.77189889\n",
      " -0.77177835]\n",
      "---------------------------------------------------------------------------\n",
      "Pre =  [-0.85766787 -0.85775569 -0.85763948 ..., -0.8577251  -0.85747949\n",
      " -0.85737852]\n",
      "---------------------------------------------------------------------------\n",
      "Pre =  [-0.95754527 -0.95765552 -0.95767489 ..., -0.95741587 -0.95739261\n",
      " -0.95715889]\n",
      "---------------------------------------------------------------------------\n",
      "Pre =  [-0.80732544 -0.80742719 -0.80739486 ..., -0.80692001 -0.8069342\n",
      " -0.80200713]\n",
      "---------------------------------------------------------------------------\n",
      "Pre =  [-0.99254743 -0.9926397  -0.99248979 ..., -0.9924593  -0.99214568\n",
      " -0.99256556]\n",
      "---------------------------------------------------------------------------\n",
      "Pre =  [-0.66325672 -0.66339462 -0.6633728  ..., -0.66322507 -0.66325388\n",
      " -0.66334805]\n",
      "---------------------------------------------------------------------------\n",
      "Pre =  [-0.96451432 -0.9645295  -0.96446516 ..., -0.9644145  -0.96413585\n",
      " -0.96324296]\n",
      "---------------------------------------------------------------------------\n",
      "Pre =  [-0.82209373 -0.82222536 -0.82205532 ..., -0.82212619 -0.82180241\n",
      " -0.74592755]\n",
      "---------------------------------------------------------------------------\n",
      "Pre =  [-0.816457   -0.81655762 -0.81656039 ..., -0.81639459 -0.81629795\n",
      " -0.81621881]\n",
      "---------------------------------------------------------------------------\n",
      "Pre =  [-0.98571511 -0.98587171 -0.98579616 ..., -0.98588096 -0.98575315\n",
      " -0.98568828]\n",
      "---------------------------------------------------------------------------\n",
      "Pre =  [-0.96711616 -0.96722438 -0.96721358 ..., -0.96693263 -0.9666972\n",
      " -0.96687294]\n",
      "---------------------------------------------------------------------------\n",
      "Pre =  [-0.98595891 -0.98600728 -0.98575252 ..., -0.98597917 -0.98550211\n",
      " -0.98583373]\n",
      "---------------------------------------------------------------------------\n",
      "Pre =  [-0.94101276 -0.94105919 -0.94098281 ..., -0.94103599 -0.76461425\n",
      " -0.55211952]\n",
      "---------------------------------------------------------------------------\n",
      "Pre =  [-0.90871332 -0.90887582 -0.90866716 ..., -0.90861048 -0.9089545  -0.9084455 ]\n",
      "---------------------------------------------------------------------------\n",
      "Pre =  [-0.91569364 -0.91570815 -0.91555877 ..., -0.91308759 -0.91524936\n",
      " -0.91255617]\n",
      "---------------------------------------------------------------------------\n",
      "Pre =  [-0.88525957 -0.88530207 -0.88517344 ..., -0.88514444 -0.88515382\n",
      " -0.8849931 ]\n",
      "---------------------------------------------------------------------------\n",
      "Pre =  [-0.84998669 -0.85005984 -0.8499134  ..., -0.84978644 -0.8494013  -0.8496263 ]\n",
      "---------------------------------------------------------------------------\n",
      "Pre =  [-0.96145875 -0.96148671 -0.96133461 ..., -0.96142117 -0.96092307\n",
      " -0.96101827]\n",
      "---------------------------------------------------------------------------\n",
      "Pre =  [-0.90288326 -0.9029537  -0.90298607 ..., -0.90278709 -0.90273837\n",
      " -0.90274729]\n",
      "---------------------------------------------------------------------------\n",
      "Pre =  [-0.89101258 -0.89109059 -0.89088501 ..., -0.89076702 -0.89046221\n",
      " -0.88370078]\n",
      "---------------------------------------------------------------------------\n",
      "Pre =  [-0.9754427  -0.97547658 -0.97535725 ..., -0.97515225 -0.97502072\n",
      " -0.97515583]\n",
      "---------------------------------------------------------------------------\n",
      "Pre =  [-0.86908386 -0.8690997  -0.86896259 ..., -0.86793852 -0.86878103\n",
      " -0.76348597]\n",
      "---------------------------------------------------------------------------\n",
      "Pre =  [-0.95497503 -0.95517155 -0.95506273 ..., -0.95502381 -0.95488411\n",
      " -0.95474862]\n",
      "---------------------------------------------------------------------------\n",
      "Pre =  [-0.9948225  -0.99503053 -0.99488997 ..., -0.99471572 -0.99488796\n",
      " -0.92127748]\n",
      "---------------------------------------------------------------------------\n",
      "Pre =  [-0.89166102 -0.89184004 -0.8918127  ..., -0.79152743 -0.89169716\n",
      " -0.89047155]\n",
      "---------------------------------------------------------------------------\n",
      "Pre =  [-0.98321439 -0.9834019  -0.98324555 ..., -0.98295758 -0.98299752\n",
      " -0.98305055]\n",
      "---------------------------------------------------------------------------\n",
      "Pre =  [-0.92059455 -0.92065479 -0.92059588 ..., -0.9202294  -0.92029056\n",
      " -0.92047401]\n",
      "---------------------------------------------------------------------------\n",
      "Pre =  [-0.95823179 -0.95839651 -0.95823324 ..., -0.95834771 -0.95826022\n",
      " -0.95806797]\n",
      "---------------------------------------------------------------------------\n",
      "Pre =  [-0.9696491  -0.9697266  -0.96954894 ..., -0.96947197 -0.96948688\n",
      " -0.96029173]\n",
      "---------------------------------------------------------------------------\n",
      "Pre =  [-0.98693923 -0.98697535 -0.98694329 ..., -0.98684132 -0.98638904\n",
      " -0.98660695]\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "score = [] # without normalize as 'multiPredict.m' of the origin work\n",
    "for model in models:\n",
    "    bp_res = binaryPredict(A, model)\n",
    "    score.append(bp_res)\n",
    "    print 'Pre = ', bp_res\n",
    "    #print 'Sgn = ', np.unique(np.sign(bp_res), return_counts=True)\n",
    "    print '---------------------------------------------------------------------------'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([1, 2, 3, 4, 5, 6]), array([49242, 25391,  7272,  1586,   217,    34], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "# get label count of each node: in Evaluation.m\n",
    "lCounts = np.squeeze(np.asarray(np.sum(T, axis=1))).astype(int)\n",
    "print np.unique(lCounts, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n2l_sc = np.squeeze(np.asarray(np.matrix(score).T))\n",
    "\n",
    "pred_label = []\n",
    "for i in range(len(n2l_sc)):\n",
    "    l_count = lCounts[i]\n",
    "    sc = n2l_sc[i]\n",
    "    pred_label_i = np.array([sc.argsort()[-l_count:]]).astype(int)\n",
    "    pred_label.append(pred_label_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "Pred_L = np.zeros(shape = T.shape)\n",
    "i = 0\n",
    "for ans in pred_label:\n",
    "    Pred_L[i][ans[0]] = 1\n",
    "    i += 1\n",
    "print Pred_L\n",
    "print T.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.424331699894\n",
      "0.394406555807\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print f1_score(T.toarray(), Pred_L, average='macro')  \n",
    "print f1_score(T.toarray(), Pred_L, average='micro') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
